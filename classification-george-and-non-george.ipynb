{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12398673,"sourceType":"datasetVersion","datasetId":7818745},{"sourceId":12398681,"sourceType":"datasetVersion","datasetId":7818753}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Классификация на Георгия и не Георгия","metadata":{}},{"cell_type":"markdown","source":"## Подготовка данных для дальнейшего построение модели","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T20:22:37.385513Z","iopub.execute_input":"2025-07-07T20:22:37.385789Z","iopub.status.idle":"2025-07-07T20:22:51.597433Z","shell.execute_reply.started":"2025-07-07T20:22:37.385762Z","shell.execute_reply":"2025-07-07T20:22:51.596299Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_g = pd.read_csv(\"/kaggle/input/george/georges.csv\", header=None, names=[\"image_url\"])\ndf_ng = pd.read_csv(\"/kaggle/input/non-george/non_georges.csv\", header=None, names=[\"image_url\"])\ndf_g[\"label\"]  = 1\ndf_ng[\"label\"] = 0\ndf = pd.concat([df_g, df_ng], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T20:23:14.200170Z","iopub.execute_input":"2025-07-07T20:23:14.200486Z","iopub.status.idle":"2025-07-07T20:23:14.366639Z","shell.execute_reply.started":"2025-07-07T20:23:14.200463Z","shell.execute_reply":"2025-07-07T20:23:14.365821Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_g.shape, df_ng.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:38:57.323220Z","iopub.execute_input":"2025-07-07T22:38:57.323422Z","iopub.status.idle":"2025-07-07T22:38:57.328442Z","shell.execute_reply.started":"2025-07-07T22:38:57.323407Z","shell.execute_reply":"2025-07-07T22:38:57.327908Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"((2681, 2), (3366, 2))"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"Дисбаланса классов нет","metadata":{}},{"cell_type":"code","source":"df_train, df_test = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"label\"],\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T20:23:18.164587Z","iopub.execute_input":"2025-07-07T20:23:18.164895Z","iopub.status.idle":"2025-07-07T20:23:18.180253Z","shell.execute_reply.started":"2025-07-07T20:23:18.164870Z","shell.execute_reply":"2025-07-07T20:23:18.179372Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nimport torchvision.transforms as T\n\ndef compute_mean_std(dataset, batch_size=64):\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    total_pixels = 0\n\n    for imgs, _ in loader:\n        imgs = imgs.view(imgs.size(0), imgs.size(1), -1)\n        total_pixels += imgs.size(0) * imgs.size(2)\n        mean += imgs.sum(dim=[0, 2])\n        std  += (imgs ** 2).sum(dim=[0, 2])\n\n    mean /= total_pixels\n    std = (std / total_pixels - mean ** 2).sqrt()\n    return mean.tolist(), std.tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:39:34.412888Z","iopub.execute_input":"2025-07-07T22:39:34.413147Z","iopub.status.idle":"2025-07-07T22:39:34.419623Z","shell.execute_reply.started":"2025-07-07T22:39:34.413128Z","shell.execute_reply":"2025-07-07T22:39:34.418834Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\nsum_rgb  = torch.zeros(3)\nsum2_rgb = torch.zeros(3)\nn_pixels = 0\n\nfor url in tqdm(df_train[\"image_url\"]):\n    try:\n        response = requests.get(url, timeout=5)\n        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        tensor = transform(img)  # [3, 224, 224]\n        n_pixels += tensor.shape[1] * tensor.shape[2]\n\n        sum_rgb  += tensor.sum(dim=[1, 2])\n        sum2_rgb += (tensor ** 2).sum(dim=[1, 2])\n    except Exception as e:\n        print(f\"Ошибка с {url}: {e}\")\n        continue\n\nmean = sum_rgb / n_pixels\nstd  = (sum2_rgb / n_pixels - mean**2).sqrt()\n\nprint(\"Mean:\", mean.tolist())\nprint(\"Std: \", std.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:41:11.893511Z","iopub.execute_input":"2025-07-07T22:41:11.893765Z","iopub.status.idle":"2025-07-07T22:46:54.913021Z","shell.execute_reply.started":"2025-07-07T22:41:11.893749Z","shell.execute_reply":"2025-07-07T22:46:54.912324Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4838/4838 [05:42<00:00, 14.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean: [0.5541272759437561, 0.4881589412689209, 0.4147355556488037]\nStd:  [0.27362990379333496, 0.2739166021347046, 0.27786025404930115]\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"train_transform = test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:01.354728Z","iopub.execute_input":"2025-07-07T22:48:01.355021Z","iopub.status.idle":"2025-07-07T22:48:01.359742Z","shell.execute_reply.started":"2025-07-07T22:48:01.355000Z","shell.execute_reply":"2025-07-07T22:48:01.359002Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"class GeorgeDataset(Dataset):\n    def __init__(self, df, transform = None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        url   = self.df.loc[idx, \"image_url\"]\n        label = self.df.loc[idx, \"label\"]\n        try:\n            resp = requests.get(url, timeout=5)\n            img  = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n        except:\n            img = Image.new(\"RGB\", (224,224), (0,0,0))\n        if self.transform:\n            img = self.transform(img)\n        return img, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:03.975672Z","iopub.execute_input":"2025-07-07T22:48:03.976155Z","iopub.status.idle":"2025-07-07T22:48:03.981848Z","shell.execute_reply.started":"2025-07-07T22:48:03.976132Z","shell.execute_reply":"2025-07-07T22:48:03.981096Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"train_dataset = GeorgeDataset(df_train, transform=train_transform)\ntest_dataset  = GeorgeDataset(df_test,  transform=test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:06.262259Z","iopub.execute_input":"2025-07-07T22:48:06.262855Z","iopub.status.idle":"2025-07-07T22:48:06.267908Z","shell.execute_reply.started":"2025-07-07T22:48:06.262832Z","shell.execute_reply":"2025-07-07T22:48:06.267375Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:08.167811Z","iopub.execute_input":"2025-07-07T22:48:08.168062Z","iopub.status.idle":"2025-07-07T22:48:08.172474Z","shell.execute_reply.started":"2025-07-07T22:48:08.168045Z","shell.execute_reply":"2025-07-07T22:48:08.171821Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"## Бинарная классификация","metadata":{}},{"cell_type":"markdown","source":"Попробуем для начала обучить модель для бинарной классификации. Рассмотрен предобученную модель - **EfficientNet**.","metadata":{}},{"cell_type":"code","source":"model = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-4)\n\nmodel = model.to(device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T19:08:59.783353Z","iopub.execute_input":"2025-07-07T19:08:59.783627Z","iopub.status.idle":"2025-07-07T19:08:59.945320Z","shell.execute_reply.started":"2025-07-07T19:08:59.783607Z","shell.execute_reply":"2025-07-07T19:08:59.944761Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 10\n\nfor epoch in range(1, num_epochs+1):\n    model.train()\n    running_loss = 0\n    running_corrects = 0\n    total = 0\n\n    for imgs, labels in tqdm(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(dim=1)\n        running_loss += loss.item() * imgs.size(0)\n        running_corrects += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n    epoch_loss = running_loss / total\n    epoch_acc  = running_corrects / total\n\n    model.eval()\n    test_loss = 0\n    test_corrects = 0\n    test_total = 0\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(test_loader, leave=False):\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n            preds = outputs.argmax(dim=1)\n            test_loss += loss.item() * imgs.size(0)\n            test_corrects += (preds == labels).sum().item()\n            test_total += imgs.size(0)\n\n    epoch_test_loss = test_loss / test_total\n    epoch_test_acc  = test_corrects / test_total\n\n    print(\n        f\"Epoch {epoch}/{num_epochs} — \"\n        f\"Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f} | \"\n        f\"Test  loss: {epoch_test_loss:.4f}, acc: {epoch_test_acc:.4f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T19:09:02.138659Z","iopub.execute_input":"2025-07-07T19:09:02.139369Z","iopub.status.idle":"2025-07-07T19:31:19.780140Z","shell.execute_reply.started":"2025-07-07T19:09:02.139337Z","shell.execute_reply":"2025-07-07T19:31:19.779311Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 152/152 [01:36<00:00,  1.57it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 — Train loss: 0.3348, acc: 0.8507 | Test  loss: 0.2005, acc: 0.9231\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:34<00:00,  1.61it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 — Train loss: 0.1377, acc: 0.9508 | Test  loss: 0.1863, acc: 0.9322\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:32<00:00,  1.65it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 — Train loss: 0.0737, acc: 0.9760 | Test  loss: 0.1802, acc: 0.9421\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:46<00:00,  1.42it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 — Train loss: 0.0503, acc: 0.9822 | Test  loss: 0.2009, acc: 0.9421\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:34<00:00,  1.61it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 — Train loss: 0.0485, acc: 0.9843 | Test  loss: 0.2099, acc: 0.9421\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:32<00:00,  1.63it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 — Train loss: 0.0316, acc: 0.9899 | Test  loss: 0.2368, acc: 0.9372\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [02:02<00:00,  1.24it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 — Train loss: 0.0511, acc: 0.9802 | Test  loss: 0.2677, acc: 0.9339\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:38<00:00,  1.54it/s]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 — Train loss: 0.0278, acc: 0.9899 | Test  loss: 0.2492, acc: 0.9430\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [02:59<00:00,  1.18s/it]\n                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 — Train loss: 0.0193, acc: 0.9921 | Test  loss: 0.2332, acc: 0.9397\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 152/152 [01:36<00:00,  1.57it/s]\n                                               ","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 — Train loss: 0.0191, acc: 0.9921 | Test  loss: 0.2569, acc: 0.9413\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Как мы можем заменить модель довольно плохо учиться. Конечно, мы могли бы воспользоваться более лучшими моделями или воспользоваться трюками, которые бы увеличало качество, такими как, к примеру, аугминтация, но стоит также отметить, что учить бинарную задачу для классификации класс и не класс не совсем корректно, поскольку весь негативный класс крайне разнородный и по сути включает в себя «всё, что угодно, кроме одного конкретного класса». В таких условиях бинарная классификация превращается в задачу с плохо определённой границей между классами, и модель начинает запоминать специфические особенности негативных примеров, а не обобщать отличительные черты позитивного класса. Это приводит к переобучению.\n\nДальнейшем попробуем другие модели для решение нашей задач.","metadata":{}},{"cell_type":"markdown","source":"## Получение эмбедингов","metadata":{}},{"cell_type":"markdown","source":"Попробуем проебразовать изображение в векторное представление, а уже потом работать с ними. В качестве модели для извлечение эмбедингов возьмом не модель для классификации без последнего слоя, а CLIPE, поскольку CLIP обучен на огромном количестве разнообразных изображений и текстов, и его визуальная часть формирует семантически насыщенные эмбеддинги, хорошо отражающие смысл изображения. Это позволяет эффективно использовать CLIP в задачах, где важна смысловая близость, а не только формальное сходство, как в случае с нашей задачей: обнаружить конкретный класс среди произвольных объектов.\n\n\nCLIP по своей природе представляет модель, обученной на парных данных: изображения и их текстовые описания. Она оптимизирует контрастивный InfoNCE-лосс, минимизируя расстояние (в косинусной метрике) между эмбеддингами соответствующих изображений и текстов, и одновременно максимизируя расстояние между несоответствующими парами. Благодаря этому CLIP формирует общее латентное пространство, в котором семантически близкие объекты (например, изображения одного класса) оказываются рядом, а объекты разных классов — далеко друг от друга. Такая природа обучения делает эмбеддинги CLIP сразу пригодными для задач, основанных на расстояниях, без дополнительного обучения: k-NN, Mahalanobis, One-Class SVM и другие. Особенно это полезно в сценариях одноклассовой классификации, где нужно отличать позитивные примеры от негативных без явного наличия всех классов в обучающей выборке. Поскольку CLIP уже эффективно разводит разные классы в пространстве, его эмбеддинги позволяют использовать расстояние до положительного эталона (или центра кластера) как меру принадлежности к классу — что делает CLIP мощным инструментом в задачах обнаружения аномалий и одноклассового анализа.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:16.944805Z","iopub.execute_input":"2025-07-07T22:48:16.945482Z","iopub.status.idle":"2025-07-07T22:48:16.949422Z","shell.execute_reply.started":"2025-07-07T22:48:16.945436Z","shell.execute_reply":"2025-07-07T22:48:16.948888Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"class CLIPDataset(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        url   = self.df.loc[idx, \"image_url\"]\n        label = int(self.df.loc[idx, \"label\"])\n        try:\n            img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n        except:\n            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n        return img, label\n\ndef collate_fn(batch):\n    imgs, labels = zip(*batch)\n    return list(imgs), torch.tensor(labels, dtype=torch.long)\n\ndf_g  = pd.read_csv(\"/kaggle/input/george/georges.csv\", header=None, names=[\"image_url\"])\ndf_ng = pd.read_csv(\"/kaggle/input/non-george/non_georges.csv\", header=None, names=[\"image_url\"])\ndf_g[\"label\"], df_ng[\"label\"] = 1, 0\ndf = pd.concat([df_g, df_ng], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\ndf_train = df.sample(frac=0.8, random_state=42)\ndf_test  = df.drop(df_train.index)\n\ntrain_ds = CLIPDataset(df_train)\ntest_ds  = CLIPDataset(df_test)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:48:19.577401Z","iopub.execute_input":"2025-07-07T22:48:19.577716Z","iopub.status.idle":"2025-07-07T22:48:19.613832Z","shell.execute_reply.started":"2025-07-07T22:48:19.577694Z","shell.execute_reply":"2025-07-07T22:48:19.613009Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"processor  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_clip_embeddings(loader):\n    X, y = [], []\n    with torch.no_grad():\n        for imgs, labels in tqdm(loader):\n            inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n            feats  = clip_model.get_image_features(**inputs)\n            emb    = F.normalize(feats, p=2, dim=1).cpu().numpy()\n            X.append(emb)\n            y.extend(labels.numpy())\n    return np.vstack(X), np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:30:58.180352Z","iopub.execute_input":"2025-07-07T22:30:58.180951Z","iopub.status.idle":"2025-07-07T22:30:58.185849Z","shell.execute_reply.started":"2025-07-07T22:30:58.180925Z","shell.execute_reply":"2025-07-07T22:30:58.185174Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"X_train, y_train = extract_clip_embeddings(train_loader)\nX_test,  y_test  = extract_clip_embeddings(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:30:59.939629Z","iopub.execute_input":"2025-07-07T22:30:59.940097Z","iopub.status.idle":"2025-07-07T22:38:57.321955Z","shell.execute_reply.started":"2025-07-07T22:30:59.940075Z","shell.execute_reply":"2025-07-07T22:38:57.321332Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 152/152 [06:20<00:00,  2.51s/it]\n100%|██████████| 38/38 [01:36<00:00,  2.54s/it]\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"### Обучение линейных классификаторов\n\nПопробуем обучить простые модели для классификации поверх эмбедингов наших изображений и сравним их работу","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import normalize\n\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=200,\n        class_weight=\"balanced\",\n        random_state=42\n    ),\n    \"SVM\": SVC(\n        kernel=\"rbf\",\n        probability=True,\n        class_weight=\"balanced\",\n        random_state=42\n    ),\n    \"XGBoost\": XGBClassifier(\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42\n    ),\n    \"MLP\": MLPClassifier(\n        hidden_layer_sizes=(128, 64),\n        max_iter=300,\n        random_state=42\n    ),\n    \"k-NN\": KNeighborsClassifier(\n        n_neighbors=1,\n        metric=\"cosine\"\n    )\n}\n\nfor name, clf in models.items():\n    print(f\"\\n==== {name} ====\")\n    clf.fit(X_train, y_train)\n\n    y_pred  = clf.predict(X_test)\n\n    if hasattr(clf, \"predict_proba\"):\n        y_score = clf.predict_proba(X_test)[:, 1]\n        roc = roc_auc_score(y_test, y_score)\n    else:\n        y_score = y_pred  # fallback\n        roc = roc_auc_score(y_test, y_pred)\n\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    print(f\"ROC-AUC:  {roc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:18:38.782933Z","iopub.execute_input":"2025-07-07T22:18:38.783164Z","iopub.status.idle":"2025-07-07T22:19:14.996597Z","shell.execute_reply.started":"2025-07-07T22:18:38.783148Z","shell.execute_reply":"2025-07-07T22:19:14.995312Z"}},"outputs":[{"name":"stdout","text":"\n==== LogisticRegression ====\nAccuracy: 0.9338\nROC-AUC:  0.9777\n\n==== RandomForest ====\nAccuracy: 0.9231\nROC-AUC:  0.9796\n\n==== SVM ====\nAccuracy: 0.9487\nROC-AUC:  0.9845\n\n==== XGBoost ====\nAccuracy: 0.9338\nROC-AUC:  0.9829\n\n==== MLP ====\nAccuracy: 0.9512\nROC-AUC:  0.9855\n\n==== k-NN ====\nAccuracy: 0.9305\nROC-AUC:  0.9319\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import normalize\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import normalize\n\nfrom sklearn.preprocessing import StandardScaler\n\n### попробуем нормализовать векторы\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n\nmodels = {\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=200,\n        class_weight=\"balanced\",\n        random_state=42\n    ),\n    \"SVM\": SVC(\n        kernel=\"rbf\",\n        probability=True,\n        class_weight=\"balanced\",\n        random_state=42\n    ),\n    \"XGBoost\": XGBClassifier(\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        random_state=42\n    ),\n    \"MLP\": MLPClassifier(\n        hidden_layer_sizes=(128, 64),\n        max_iter=300,\n        random_state=42\n    ),\n    \"k-NN\": KNeighborsClassifier(\n        n_neighbors=1,\n        metric=\"cosine\"\n    )\n}\n\nfor name, clf in models.items():\n    print(f\"\\n==== {name} ====\")\n    clf.fit(X_train, y_train)\n\n    y_pred  = clf.predict(X_test)\n\n    if hasattr(clf, \"predict_proba\"):\n        y_score = clf.predict_proba(X_test)[:, 1]\n        roc = roc_auc_score(y_test, y_score)\n    else:\n        y_score = y_pred  # fallback\n        roc = roc_auc_score(y_test, y_pred)\n\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    print(f\"ROC-AUC:  {roc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T22:25:04.252754Z","iopub.execute_input":"2025-07-07T22:25:04.253554Z","iopub.status.idle":"2025-07-07T22:25:42.921541Z","shell.execute_reply.started":"2025-07-07T22:25:04.253521Z","shell.execute_reply":"2025-07-07T22:25:42.920828Z"}},"outputs":[{"name":"stdout","text":"\n==== LogisticRegression ====\nAccuracy: 0.9330\nROC-AUC:  0.9762\n\n==== RandomForest ====\nAccuracy: 0.9231\nROC-AUC:  0.9796\n\n==== SVM ====\nAccuracy: 0.9529\nROC-AUC:  0.9867\n\n==== XGBoost ====\nAccuracy: 0.9338\nROC-AUC:  0.9829\n\n==== MLP ====\nAccuracy: 0.9529\nROC-AUC:  0.9842\n\n==== k-NN ====\nAccuracy: 0.9371\nROC-AUC:  0.9368\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"### Выводы\n\nВ ходе экспериментов были протестированы различные подходы к одноклассовай классификации. Наилучшие результаты по метрике ROC-AUC и accuracy показала полносвязная нейронная сеть (MLP) на эмбидингах, получинных при помощи Clip, что может быть объяснено её способностью эффективно извлекать сложные нелинейные зависимости в эмбеддинговом пространстве. В отличие от одноклассовых подходов, которые оптимизируют геометрическую компактность положительного класса, MLP получает информацию о границе между классами напрямую через бинарные метки, что позволяет ей формировать более точную разделяющую поверхность. Особенно важно отметить, что использование нормализованных эмбеддингов CLIP усиливает эффект семантической разреженности между классами, что дополнительно способствует высокому качеству классификации при использовании моделей, чувствительных к расстоянию (таких как k-NN и MLP). Несмотря на то, что одноклассовые методы концептуально лучше подходят для задач с ограниченным числом классов, в условиях доступности меток и разнообразия классов в тренировочной выборке прямое обучение бинарного классификатора, такого как MLP, может оказаться более эффективным и практически обоснованным решением.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}